{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Для убирания пробелов и знаков препинания буду использовать re.findall, поэтому re импортируем. !ВАЖНО! я не знала как лучше сделать - внести импорт в функцию или нет - но решила не вносить, чтобы не было постоянной переустановки при прогоне итогового кода\n",
        "\n",
        "def simple_tokenization(texts): # Создаю функцию (я переименовала переменную с текстами в texts, потому что мне морально легче, так точно вижу, что там тексты, а не текст)\n",
        "  for text in texts: # Перебираем каждый текст\n",
        "    texts = re.findall(r\"\\w+\", texts) # Удаляем всё нам ненужное\n",
        "    return texts # Возращаем токенизированную красоту\n",
        "\n",
        "for text in texts: # Попытки написания кода показали, что лучше проверять функции перед сведением итогового кода. Поэтому от себя в код я добавляю проверку.\n",
        "  print(simple_tokenization(text)) # Проверка успешна!"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe3ae97-1baf-4d48-e94e-d690b1e14fae"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'It', 's', 'a', 'beautiful', 'day']\n",
            "['Dr', 'Smith', 'arrived', 'at', '5', '30', 'p', 'm', 'from', 'New', 'York', 'The', 'meeting', 'cost', '1', '000', '50']\n",
            "['I', 'can', 't', 'believe', 'she', 's', 'going', 'Let', 's', 'meet', 'at', 'Jane', 's', 'house', 'They', 'll', 'love', 'it']\n",
            "['What', 's', 'the', 'ETA', 'for', 'the', 'package', 'Please', 'e', 'mail', 'support', 'example', 'com', 'ASAP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # Импортируем все нужное\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def nltk_tokenization(texts): # Пишем функцию\n",
        "  return (word_tokenize(text)) # Возращаем результат\n",
        "\n",
        "for text in texts: # Проверяем...\n",
        "  print(nltk_tokenization(text)) # Проверка успешна!"
      ],
      "metadata": {
        "id": "14BIv33iqrkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b86cf7-9809-4754-fdce-7510accaa1e0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy # Импортируем нужное\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_tokenization(texts): # Пишем функцию\n",
        "  for text in texts:\n",
        "    doc = nlp(texts)\n",
        "\n",
        "  return [t.text for t in doc] # Возращаем результат (немного некрасиво выглядит, зато без доп. переменной)\n",
        "\n",
        "for text in texts: # Проверяем...\n",
        "  print(spacy_tokenization(text)) # Проверка успешна!"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ea794e5-724a-4508-fd54-a195c454ac60"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e6f163-f407-4d3a-f46a-2de08dc8b427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предложение для токенизации: The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "Простая токенизация: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'It', 's', 'a', 'beautiful', 'day']\n",
            "Токенизация с помощью NLTK: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "Токенизация с помощью Spacy: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Токенизация проведена успешно!\n",
            "Предложение для токенизации: Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\n",
            "Простая токенизация: ['Dr', 'Smith', 'arrived', 'at', '5', '30', 'p', 'm', 'from', 'New', 'York', 'The', 'meeting', 'cost', '1', '000', '50']\n",
            "Токенизация с помощью NLTK: ['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "Токенизация с помощью Spacy: ['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Токенизация проведена успешно!\n",
            "Предложение для токенизации: I can't believe she's going! Let's meet at Jane's house. They'll love it.\n",
            "Простая токенизация: ['I', 'can', 't', 'believe', 'she', 's', 'going', 'Let', 's', 'meet', 'at', 'Jane', 's', 'house', 'They', 'll', 'love', 'it']\n",
            "Токенизация с помощью NLTK: ['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "Токенизация с помощью Spacy: ['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Токенизация проведена успешно!\n",
            "Предложение для токенизации: What's the ETA for the package? Please e-mail support@example.com ASAP!\n",
            "Простая токенизация: ['What', 's', 'the', 'ETA', 'for', 'the', 'package', 'Please', 'e', 'mail', 'support', 'example', 'com', 'ASAP']\n",
            "Токенизация с помощью NLTK: ['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n",
            "Токенизация с помощью Spacy: ['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Токенизация проведена успешно!\n"
          ]
        }
      ],
      "source": [
        "for text in texts: # Применяем функции для каждого текста\n",
        "  print(f\"Предложение для токенизации: {text}\") # Сначала выведем само предложение\n",
        "  print(\"Простая токенизация:\", simple_tokenization(text)) # Теперь делаем простую токенизацию\n",
        "  print(\"Токенизация с помощью NLTK:\", nltk_tokenization(text)) # Токенизацию помощью NLTK\n",
        "  print(\"Токенизация с помощью Spacy:\", spacy_tokenization(text)) # И с помощью Spacy\n",
        "  print(\"---\"*60) # Делаем красоту, чтобы не приходилось напрягаться и читать результат\n",
        "  print(\"Токенизация проведена успешно!\") # Делаем вывод\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)"
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Когда мы выполняем простую токенизацию, то очень сильно упрощаем структуру языка и не учитываем множество нюансов и исключений, которые важны для понимания текста.\n",
        "Пример 1: фразовые глаголы в английском языке. Простая токенизация будет разделять глаголы по типу \"take off\", \"turn out\" и т.д. Подобное разделение приведёт к потере смысла предложения или даже всего текста.\n",
        "Пример 2: в некоторых языках в целом нет пробелов между словами. Например, китайская фраза: \"我想要十分\" переводится как \"я хочу десять баллов\". В китайском между иероглифами нет пробелов, значит и токенизировать текст по пробелам не выйдет.\n",
        "Помимо этого простая токенизация плохо обрабатывает: формулы и примеры в специализированных текста; эмотиконы и хештеги; электронные почты; термины и названия, в которых больше одного слова и многое другое.\n",
        "Таким образом, простое разделение текста по пробелам и знакам препинания во многом является недостаточным для токенизации чего-то сложного и комплексного.\n",
        "\n",
        "2. Согласно данному сайту: https://gpt-tokenizer.dev/ в фразе \"You shall know a word by the company it keeps\" 10 токенов. Для получения этого значения достаточно было вставить фразу в предложенное окно на сайте. В целом сайт довольно ожидаемо разделил слова на токены (1 слово = 1 токен)\n",
        "\n",
        "3. BPE работает на основе анализа символом, которые чаще всего встречаются и используются вместе. На основе этого он и формирует токены. Алгоритм его работы следующий:\n",
        "1) Основной текст разбивается на отдельные символы (например, \"привет\" становится \"п\", \"р\", \"и\", \"в\", \"е\", \"т\").\n",
        "2) Далее постепенно формируется словарь новых токенов путем нахождения самых частотных пар токенов. Так, сначала сливаются, предположим, \"п\" и \"р\", и получается токен \"пр\". А затем к нему по частотности встречаемости с этим токеном других добавляются остальные компоненты слова.\n",
        "3) Это не как этап, а больше как ограничение. BPE обучают на основе больших текстов, чтобы он понимал границы слова и то, в какой момент надо перестать сливать токены воедино.\n"
      ],
      "metadata": {
        "id": "C3i82oogFguN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}